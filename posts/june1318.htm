<!DOCTYPE html>

<html lang="en">
<head>
  <meta charset="utf-8">
  <title>A day In life: June 13, 2018</title>
  <link rel="stylesheet" href="../style.css" media="screen">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-119754085-1"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119754085-1');
  </script>
  <link rel="icon" href="../img/fav32.ico" type="image/gif">
</head>

<body>
  <div class="wrapper">
    <header class="header">
      <a href="../index.htm">
        <h2>
          <span class=redLetter>A</span><span class=grayLetter> day</span><span class=redLetter> I</span><span class=grayLetter>n life.</span>
        </h2>
      </a>
      <h3 class="mainTitle">Computer & Data Science with a pinch of music</h3>
      <hr>
    </header>
    <div class="content">
      <div class = "date">June 13, 2018</div>
      <h3 class="postTitle">Just enough to get you started with Apache Spark</h3>
      <h3>Introduction</h3>
      <p>
        When Big Data became a thing some years ago, I tried several frameworks,
        platforms and technologies. Most, if not all of them were not so good at hiding
        their inherent distributed nature, which caused a steep learning curve
        and increased complexity. The first thing I noticed in Apache Spark was
        the fact that it managed to create such abstractions, that it could
        hide all the ugly parts. It managed to do that in a way that you
        feel you are just manipulating in-memory data structures in some
        high-level programming language - like a HashSet in Java. You know, the usual stuff.
        This guide is meant to be a bare minimum tutorial for people who have
        never seen Spark before or has been a while since they last played with it.
      </p>
      <h3>Apache Spark</h3>
      <p>
        Apache Spark is an analytics framework in a distributed computing environment,
        originally developer at UC Berkeley in 2009 (later donated to the Apache Software Foundation).
        It supports several languages such as Python, Scala and Java and can be
        used for Big Data and Machine Learning purposes.
      </p>
      <p>
        It is important to remember that in the heart of Spark, there is the
        Resilient Distributed Dataset (RDD), the core data structure of Spark.
        We are going to talk about RDDs shortly.
        Spark comes with different components to make sure it spans across
        several fields and technologies:
        <ul>
          <li>Spark Core: Spark’s distributed execution engine.</li>
          <li>Spark SQL: Support for SQL queries.</li>
          <li>Spark Streaming: Stream processing of live data streams.</li>
          <li>GraphX: API for graphs.</li>
          <li>MLlib: Machine Learning in Spark.</li>
        </ul>
      </p>
      <img class=imageSmall src="../img/daft_spark.png" alt="Daft Spark" border="1">
      <h3>Harder, Better, Faster, Stronger (Spark vs. Hadoop MapReduce)</h3>
      <p>
        Since we already have Hadoop MapReduce, why use Apache Spark instead for data processing?
        A key difference here is that while Hadoop MapReduce uses the <b>disk</b> to persist
        intermediate results, Spark processes data <b>in-memory</b>.
        This makes Spark much faster and can be proven critical in real time
        applications such as stream processing. Spark may be up to 100 times faster.
        However, in batch processing where time is not critical but also the
        extra disk space might be handy i.e. large datasets, Hadoop MapReduce
        is still competitive. Finally Spark’s components allow for SQL queries,
        Graph algorithms and Machine Learning. To recap, what we get with Spark:
        <ul>
          <li>
            Easier programming. Because of its abstractions
            the programmer does not need to know about any internals.
          </li>
          <li>
            You get to start Spark from a Command Line Interface!
            It can be as easy as start the CLI, load a file and start processing it.
          </li>
          <li>Faster stream processing.</li>
          <li>SQL queries!</li>
        </ul>
      </p>
      <h3>RDDs</h3>
      <p>
        RDDs are fundamental data structures of Spark along with the Dataframes.
        They are an abstraction which hides a lot of complicated details.
        All you need to know is their API and not necessarily what it is going on
        under the hood. The abbreviation breaks down as:
        <ul>
          <li>
            <b>Resilient</b>, which means they are fault-tolerant:
            recompute missing or even damaged partitions due to node failures.
          </li>
          <li><b>Distributed</b>, since they act on multiple nodes in a cluster.</li>
          <li>
            <b>Dataset</b> is a collection of partitioned data with primitive values
            or values of values, e.g. tuples or other objects.
          </li>
        </ul>
      </p>
      <p>
        There are three ways to create an RDD in Spark:
        <ul>
          <li>Parallelizing an already existing collection in your driver program</li>
          <li>Referencing a dataset in an external storage system, e.g. HDFS</li>
          <li>Creating an RDD from already existing RDDs</li>
        </ul>
      </p>
      <h3>Basic RDD API</h3>
      <p>
        RDDs offer an API which can be used to handle their lifecycle.
        You can read the fairly long API from the
        <a href="https://spark.apache.org/docs/2.2.0/api/scala/index.html#package">official source</a>
        but you can get a taste with the following:
        <h4>Parallelize</h4>
        <p>
          One of the most common operations is "parallelize".
          It takes as an argument a collection such as an array and it essentially
          does what the name suggests: copies the data to a distributed setting where we can run operations in parallel.
          Let’s see what the code (Scala) looks like:
          <div class="boxed">
              1. val spark = SparkSession.builder() </br>
              2. val myRdd = spark.sparkContext.parallelize(Array(1, 2, 3, 4, 5))
          </div>
          That’s it. You created a spark context which is the very first object you
          create when developing a Spark SQL app and "parallelized" a given dataset.
        </p>
        <h4>Collect</h4>
        <p>
          Returns all the elements of the dataset as an array at the driver program
          (i.e. the master node). In some sense this is the reverse of "parallelize",
          calling the data back to the driver program.
          Note that this is a memory intensive call
          and should be used after a "filter" or similar size reducing operation,
          otherwise it can easily cause a memory exception if the dataset is too
          large to fit in memory. A safe alternative is to use "take" instead,
          in order to see a part of the data.
        </p>
      </p>

      <h3>Dataframes</h3>
      <p>
        Dataframes are another abstraction built on top of RDDs, which means they are
        an even higher abstraction. As the official Spark guide says, you can
        create DataFrames from an existing RDD, from a Hive table, or from Spark
        data sources (e.g. text file, Parquet file, JSON, etc.). Roughly, you can
        think of RDDs as operating on key/value datasets and dataframes on matrices
        (like SQL tables). If you are familiar with R, they are very similar
        abstractions with R's dataframes.
        Let’s see how to create one from a JSON file right away:
        <div class="boxed">
          1. val df = spark.read.json("path/grades.json") </br>
          2. df.createOrReplaceTempView("grades") </br>
          3. val sqlDF = spark.sql("SELECT * FROM grades") </br>
          4. sqlDF.show()
        </div>
      </p>

      <p>
        In line 1, we create a dataframe from a JSON which contains a student's grade
        along with its grade in a course. Line 2 creates a view which is the equivalent
        of a database table. We call this "grades". In line 3 we declare the SQL query we
        want to run and finally in line 4 we display the results. Simple as that!
      </p>

      <p>
        Taking it from here, this is where magic happens.
        You are free to experiment with the API and try numerous processing functions, e.g. map, reduce, filter etc.
        There is a lot going on of increasing complexity which you can pick up by consulting the official API.
      </p>
      <p>Credits to <a href="https://www.linkedin.com/in/christos-mantas-2b79a9107/">Christos Mantas</a> for sharing his expertise on Spark.
      <div class="just-comments"
      data-apikey="9fa1f1f5-2ca0-4d89-bb34-b78c48fafcb1"
      data-hideattribution="true">
    </div>
    <script async src="https://just-comments.com/w.js"></script>
  </div> <!-- content -->
  <div class="footer">
    <ul class="bottomlinks">
      <li class="bottomli"><a href="https://github.com/lovemeblender"><img src="../img/gb-64.png" alt="Github" height="30" width="30"></a></li>
      <li class="bottomli"><a href="http://uk.linkedin.com/in/mdespotopoulos/"><img src="../img/in-34.png" alt="LinkedIn" height="26" width="30"></a></li>
      <li class="bottomli"><a href="https://twitter.com/mdespotopoulos"><img src="../img/twitter.png" alt="Twitter" height="30" width="30"></a></li>
    </ul>
  </div>
</div> <!-- wrapper -->

</body>
</html>
